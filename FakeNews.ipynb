{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markosnakos/Assignement-NLP-FakeNews/blob/main/FakeNews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Cell 1 ‚Äî Install Dependencies"
      ],
      "metadata": {
        "id": "sspmPyIqhqj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp0NjmW6hgXm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0) Setup\n",
        "# ============================================================\n",
        "!pip -q install -U datasets transformers accelerate evaluate\n",
        "\n",
        "!pip -q install -U wandb\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.login()  # Œ∏Œ± Œ∂Œ∑œÑŒÆœÉŒµŒπ key œÑŒ∑ŒΩ 1Œ∑ œÜŒøœÅŒ¨\n",
        "\n",
        "wandb.init()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ============================================================\n",
        "# 1) Load datasets (GossipCop++ + PolitiFact++)\n",
        "#    Expect splits: HR, HF, MR, MF\n",
        "# ============================================================\n",
        "GC_NAME = \"Jinyan1/GossipCop\"\n",
        "PF_NAME = \"Jinyan1/PolitiFact\"\n",
        "\n",
        "def load_four_splits(ds_name):\n",
        "    hr = load_dataset(ds_name, split=\"HR\")\n",
        "    hf = load_dataset(ds_name, split=\"HF\")\n",
        "    mr = load_dataset(ds_name, split=\"MR\")\n",
        "    mf = load_dataset(ds_name, split=\"MF\")\n",
        "    return hr, hf, mr, mf\n",
        "\n",
        "gc_hr, gc_hf, gc_mr, gc_mf = load_four_splits(GC_NAME)\n",
        "pf_hr, pf_hf, pf_mr, pf_mf = load_four_splits(PF_NAME)\n",
        "\n",
        "print(\"GossipCop sizes:\", len(gc_hr), len(gc_hf), len(gc_mr), len(gc_mf))\n",
        "print(\"PolitiFact sizes:\", len(pf_hr), len(pf_hf), len(pf_mr), len(pf_mf))\n",
        "\n",
        "# ============================================================\n",
        "# 2) Add labels + subclass\n",
        "#    label: 0=real, 1=fake\n",
        "#    subclass: HR/HF/MR/MF\n",
        "# ============================================================\n",
        "def add_labels(ds, subclass):\n",
        "    is_fake = 1 if subclass in [\"HF\", \"MF\"] else 0\n",
        "    ds = ds.add_column(\"label\", [is_fake] * len(ds))\n",
        "    ds = ds.add_column(\"subclass\", [subclass] * len(ds))\n",
        "    return ds\n",
        "\n",
        "gc_hr = add_labels(gc_hr, \"HR\")\n",
        "gc_hf = add_labels(gc_hf, \"HF\")\n",
        "gc_mr = add_labels(gc_mr, \"MR\")\n",
        "gc_mf = add_labels(gc_mf, \"MF\")\n",
        "\n",
        "pf_hr = add_labels(pf_hr, \"HR\")\n",
        "pf_hf = add_labels(pf_hf, \"HF\")\n",
        "pf_mr = add_labels(pf_mr, \"MR\")\n",
        "pf_mf = add_labels(pf_mf, \"MF\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) Utilities: sampling + mixtures (paper-style settings)\n",
        "# ============================================================\n",
        "def sample_n(ds, n, seed=SEED):\n",
        "    n = min(n, len(ds))\n",
        "    return ds.shuffle(seed=seed).select(range(n))\n",
        "\n",
        "def build_train_set(setting, hr, hf, mr, mf, mf_ratio=0.0,\n",
        "                    real_size=2000, fake_size=2000, seed=SEED):\n",
        "    \"\"\"\n",
        "    setting:\n",
        "      - \"human_legacy\": real=HR\n",
        "      - \"transitional\": real=HR+MR (simple approximation)\n",
        "      - \"machine_dominance\": real=MR\n",
        "    mf_ratio: fraction of fake examples that are MF (rest HF)\n",
        "    \"\"\"\n",
        "    if setting == \"human_legacy\":\n",
        "        real_pool = hr\n",
        "    elif setting == \"transitional\":\n",
        "        real_pool = concatenate_datasets([hr, mr])\n",
        "    elif setting == \"machine_dominance\":\n",
        "        real_pool = mr\n",
        "    else:\n",
        "        raise ValueError(\"setting must be: human_legacy, transitional, machine_dominance\")\n",
        "\n",
        "    real_train = sample_n(real_pool, real_size, seed=seed)\n",
        "\n",
        "    mf_n = int(fake_size * mf_ratio)\n",
        "    hf_n = fake_size - mf_n\n",
        "    fake_train = concatenate_datasets([\n",
        "        sample_n(hf, hf_n, seed=seed + 1),\n",
        "        sample_n(mf, mf_n, seed=seed + 2),\n",
        "    ])\n",
        "\n",
        "    return concatenate_datasets([real_train, fake_train]).shuffle(seed=seed)\n",
        "\n",
        "def build_test_set(hr, hf, mr, mf, test_size_each=500, seed=SEED):\n",
        "    return concatenate_datasets([\n",
        "        sample_n(hr, test_size_each, seed=seed + 10),\n",
        "        sample_n(hf, test_size_each, seed=seed + 11),\n",
        "        sample_n(mr, test_size_each, seed=seed + 12),\n",
        "        sample_n(mf, test_size_each, seed=seed + 13),\n",
        "    ]).shuffle(seed=seed)\n",
        "\n",
        "gc_test = build_test_set(gc_hr, gc_hf, gc_mr, gc_mf, test_size_each=500)\n",
        "pf_test = build_test_set(pf_hr, pf_hf, pf_mr, pf_mf, test_size_each=200)\n",
        "\n",
        "print(\"GC test:\", len(gc_test), \"PF test:\", len(pf_test))\n",
        "\n",
        "# ============================================================\n",
        "# 4) Tokenization (T4 fast + robust)\n",
        "# ============================================================\n",
        "MODEL_NAME = \"roberta-base\"      # faster alt: \"distilroberta-base\"\n",
        "MAX_LEN = 128                    # speedup vs 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def make_input(example):\n",
        "    title = example.get(\"title\", \"\") or \"\"\n",
        "    desc  = example.get(\"description\", \"\") or \"\"\n",
        "    text  = example.get(\"text\", \"\") or \"\"\n",
        "\n",
        "    joined = (title + \" \" + desc).strip()\n",
        "    if len(joined) < 5:\n",
        "        joined = text\n",
        "    if joined is None or len(str(joined).strip()) == 0:\n",
        "        joined = \" \"\n",
        "    return {\"input_text\": joined}\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"input_text\"], truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "def prepare(ds, keep_subclass=False):\n",
        "    ds = ds.map(make_input)\n",
        "    ds = ds.map(tokenize, batched=True, remove_columns=[\"input_text\"])\n",
        "    base_cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "    if keep_subclass:\n",
        "        base_cols.append(\"subclass\")\n",
        "    keep = [c for c in base_cols if c in ds.column_names]\n",
        "    return ds.select_columns(keep)\n",
        "\n",
        "# ============================================================\n",
        "# 5) Build experiment datasets\n",
        "# ============================================================\n",
        "SETTING = \"human_legacy\"   # \"transitional\" / \"machine_dominance\"\n",
        "MF_RATIO = 0.50            # 0.0 / 0.33 / 0.5 / 0.67 / 1.0\n",
        "\n",
        "train_raw = build_train_set(\n",
        "    setting=SETTING,\n",
        "    hr=gc_hr, hf=gc_hf, mr=gc_mr, mf=gc_mf,\n",
        "    mf_ratio=MF_RATIO,\n",
        "    real_size=2000, fake_size=2000,  # increase later if you want\n",
        ")\n",
        "\n",
        "# IMPORTANT:\n",
        "# - train_ds: NO subclass (so collator won't crash)\n",
        "# - eval_ds: keep subclass for metrics, but we will remove it right before predict\n",
        "train_ds = prepare(train_raw, keep_subclass=False)\n",
        "gc_eval  = prepare(gc_test, keep_subclass=True)\n",
        "pf_eval  = prepare(pf_test, keep_subclass=True)\n",
        "\n",
        "print(\"Train columns:\", train_ds.column_names)\n",
        "print(\"Train sample:\", train_ds[0])\n",
        "assert \"input_ids\" in train_ds.column_names and \"attention_mask\" in train_ds.column_names, \"Tokenization failed!\"\n",
        "assert \"label\" in train_ds.column_names, \"Missing label!\"\n",
        "\n",
        "# ============================================================\n",
        "# 6) Metrics: overall + subclass-wise accuracy\n",
        "# ============================================================\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return acc.compute(predictions=preds, references=labels)\n",
        "\n",
        "def subclass_accuracy(trainer, eval_ds_with_subclass):\n",
        "    subclasses = eval_ds_with_subclass[\"subclass\"]\n",
        "    eval_ds = eval_ds_with_subclass.remove_columns([\"subclass\"])\n",
        "\n",
        "    out = trainer.predict(eval_ds)\n",
        "    preds = np.argmax(out.predictions, axis=-1)\n",
        "    labels = out.label_ids\n",
        "\n",
        "    results = {\"overall_acc\": float((preds == labels).mean())}\n",
        "    for sc in [\"HR\", \"HF\", \"MR\", \"MF\"]:\n",
        "        idx = [i for i, s in enumerate(subclasses) if s == sc]\n",
        "        if idx:\n",
        "            results[f\"acc_{sc}\"] = float((preds[idx] == labels[idx]).mean())\n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# 7) Train (T4 optimized; Solution-2 compatible)\n",
        "# ============================================================\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=f\"./runs_{SETTING}_mf{MF_RATIO}\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=32,      # if OOM -> 16\n",
        "    per_device_eval_batch_size=64,\n",
        "    gradient_accumulation_steps=2,       # effective batch ~64\n",
        "    num_train_epochs=5,                  # increase to 2-3 later\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=25,\n",
        "    seed=SEED,\n",
        "    fp16=True,                           # speed on T4\n",
        "    dataloader_num_workers=0,            # stable\n",
        "    remove_unused_columns=False,         # keep tensor fields\n",
        "    report_to=[\"wandb\"],                 # enable Weights & Biases\n",
        "    run_name=f\"tsoumi_{SETTING}_mf{MF_RATIO}\",                # disable wandb prompts\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=gc_eval.remove_columns([\"subclass\"]),  # safe\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n=== In-domain (GossipCop++) subclass metrics ===\")\n",
        "print(subclass_accuracy(trainer, gc_eval))\n",
        "\n",
        "print(\"\\n=== Out-of-domain (PolitiFact++) subclass metrics ===\")\n",
        "print(subclass_accuracy(trainer, pf_eval))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "id2label = {0: \"real\", 1: \"fake\"}\n",
        "\n",
        "def predict_text(text: str, model, tokenizer, max_len=128):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[0].detach().cpu().numpy()\n",
        "\n",
        "    probs = np.exp(logits) / np.exp(logits).sum()\n",
        "    pred = int(np.argmax(probs))\n",
        "\n",
        "    return {\n",
        "        \"prediction\": id2label[pred],\n",
        "        \"p_real\": float(probs[0]),\n",
        "        \"p_fake\": float(probs[1]),\n",
        "    }\n",
        "\n",
        "# Œ†Œ±œÅŒ¨Œ¥ŒµŒπŒ≥ŒºŒ±:\n",
        "my_text = \"A popular film actor appeared at the official premiere of their new movie on Friday, accompanied by the director and co-stars. During a brief interview, the actor spoke about the challenges of the role and expressed gratitude for the support received from the production team. The event proceeded as scheduled and was widely covered by established entertainment news outlets.\"\n",
        "print(predict_text(my_text, trainer.model, tokenizer, max_len=MAX_LEN))\n"
      ],
      "metadata": {
        "id": "PMkrDW8F4LMr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}