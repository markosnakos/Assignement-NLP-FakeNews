{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markosnakos/Assignement-NLP-FakeNews/blob/main/hierarchical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kneq2RTFD8Yz"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Hierarchical Fake News Classification with W&B\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install -U datasets transformers accelerate evaluate wandb\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import evaluate\n",
        "import wandb\n",
        "\n",
        "# =========================\n",
        "# 0) Setup\n",
        "# =========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "MAX_LEN = 128\n",
        "\n",
        "# =========================\n",
        "# 1) Load datasets\n",
        "# =========================\n",
        "GC_NAME = \"Jinyan1/GossipCop\"\n",
        "PF_NAME = \"Jinyan1/PolitiFact\"\n",
        "\n",
        "def load_four_splits(name):\n",
        "    return (\n",
        "        load_dataset(name, split=\"HR\"),\n",
        "        load_dataset(name, split=\"HF\"),\n",
        "        load_dataset(name, split=\"MR\"),\n",
        "        load_dataset(name, split=\"MF\"),\n",
        "    )\n",
        "\n",
        "gc_hr, gc_hf, gc_mr, gc_mf = load_four_splits(GC_NAME)\n",
        "pf_hr, pf_hf, pf_mr, pf_mf = load_four_splits(PF_NAME)\n",
        "\n",
        "# =========================\n",
        "# 2) Tokenization utilities\n",
        "# =========================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "def make_input(ex):\n",
        "    title = ex.get(\"title\", \"\") or \"\"\n",
        "    desc  = ex.get(\"description\", \"\") or \"\"\n",
        "    text  = ex.get(\"text\", \"\") or \"\"\n",
        "    joined = (title + \" \" + desc).strip()\n",
        "    return {\"input_text\": joined if len(joined) > 5 else text or \" \"}\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"input_text\"], truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "def prepare(ds):\n",
        "    ds = ds.map(make_input)\n",
        "    ds = ds.map(tokenize, batched=True, remove_columns=[\"input_text\"])\n",
        "    return ds.select_columns([\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# =========================\n",
        "# 3) Dataset builders\n",
        "# =========================\n",
        "def add_label(ds, label):\n",
        "    return ds.add_column(\"label\", [label] * len(ds))\n",
        "\n",
        "# ---- Authorship (Human vs Machine)\n",
        "auth_train = concatenate_datasets([\n",
        "    add_label(gc_hr, 0), add_label(gc_hf, 0),\n",
        "    add_label(gc_mr, 1), add_label(gc_mf, 1),\n",
        "]).shuffle(seed=SEED)\n",
        "\n",
        "auth_test = concatenate_datasets([\n",
        "    add_label(pf_hr, 0), add_label(pf_hf, 0),\n",
        "    add_label(pf_mr, 1), add_label(pf_mf, 1),\n",
        "]).shuffle(seed=SEED)\n",
        "\n",
        "auth_train, auth_test = prepare(auth_train), prepare(auth_test)\n",
        "\n",
        "# ---- Human veracity (HR vs HF)\n",
        "human_train = concatenate_datasets([\n",
        "    add_label(gc_hr, 0), add_label(gc_hf, 1),\n",
        "]).shuffle(seed=SEED)\n",
        "\n",
        "human_test = concatenate_datasets([\n",
        "    add_label(pf_hr, 0), add_label(pf_hf, 1),\n",
        "]).shuffle(seed=SEED)\n",
        "\n",
        "human_train, human_test = prepare(human_train), prepare(human_test)\n",
        "\n",
        "# ---- Machine veracity (MR vs MF)\n",
        "machine_train = concatenate_datasets([\n",
        "    add_label(gc_mr, 0), add_label(gc_mf, 1),\n",
        "]).shuffle(seed=SEED)\n",
        "\n",
        "machine_test = concatenate_datasets([\n",
        "    add_label(pf_mr, 0), add_label(pf_mf, 1),\n",
        "]).shuffle(seed=SEED)\n",
        "\n",
        "machine_train, machine_test = prepare(machine_train), prepare(machine_test)\n",
        "\n",
        "# =========================\n",
        "# 4) Training utilities\n",
        "# =========================\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return acc.compute(predictions=preds, references=labels)\n",
        "\n",
        "def train_model(train_ds, eval_ds, run_name):\n",
        "    wandb.init(\n",
        "        project=\"hierarchical-fake-news\",\n",
        "        name=run_name,\n",
        "        config={\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"max_len\": MAX_LEN,\n",
        "            \"epochs\": 5,\n",
        "            \"batch_size\": 32,\n",
        "            \"lr\": 3e-5,\n",
        "        },\n",
        "        reinit=True,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=2\n",
        "    )\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./{run_name}\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=5,\n",
        "        fp16=True,\n",
        "        logging_steps=50,\n",
        "        seed=SEED,\n",
        "        report_to=[\"wandb\"],\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    wandb.finish()\n",
        "    return trainer\n",
        "\n",
        "# =========================\n",
        "# 5) Train all models\n",
        "# =========================\n",
        "trainer_auth = train_model(auth_train, auth_test, \"Stage1_Authorship\")\n",
        "trainer_human = train_model(human_train, human_test, \"Stage2_Human_Veracity\")\n",
        "trainer_machine = train_model(machine_train, machine_test, \"Stage2_Machine_Veracity\")\n",
        "\n",
        "# =========================\n",
        "# 6) Hierarchical inference\n",
        "# =========================\n",
        "def hierarchical_predict(dataset):\n",
        "    auth_out = trainer_auth.predict(dataset)\n",
        "    auth_preds = np.argmax(auth_out.predictions, axis=-1)\n",
        "\n",
        "    final_preds = []\n",
        "    for i, a in enumerate(auth_preds):\n",
        "        sample = dataset.select([i])\n",
        "        if a == 0:\n",
        "            out = trainer_human.predict(sample)\n",
        "            final_preds.append(\"HR\" if np.argmax(out.predictions) == 0 else \"HF\")\n",
        "        else:\n",
        "            out = trainer_machine.predict(sample)\n",
        "            final_preds.append(\"MR\" if np.argmax(out.predictions) == 0 else \"MF\")\n",
        "    return final_preds\n",
        "\n",
        "print(\"Hierarchical pipeline ready.\")\n",
        "\n"
      ]
    }
  ]
}